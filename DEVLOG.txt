****

My goal now is to capture video with OpenCV but show it with Qt...
Capture starts now. No camera selection yet (default camera) - that's for later.

Now: how to display? I should be able to display one of the several stages of the video filters and draw stuff on top. Custom widget seems apropriate.

Or may be: QGr.View with all them layers like?

The structure should be something like:
- video is a root item which reacts on resizing
    - under the root are pixmap items and overlay graphics
- HUD is a different root item which reacts on resizing differently
    - under the HUD are HUD graphics


To display an image I need to convert it from OpenCV's Mat to Qt's QPixmap (or similar).
The simplest is if I convert Mat to some well known format first.

continue modularization:

* * *

[x] Detect and show eyes
    ( ) Better: given two sets of potential eyes find best pair:
        ( ) pair scores: size difference, y difference
        --- works for now, but eats a lot of CPU.
[ ] Transform / crop the face to normalize the eyes distance / orientation
    - find the rotated square around the face (from the eyes down then grow / shift)
    - find the affine matrix to get it into 100x100
    - warpAffine(... )
[ ] Move view controls to the dock
[ ] Add min face size control to the dock
[ ] Display current frame sizes and FPS somewhere
[ ] Offline face learner
[ ] Online face matcher in a separate thread

Detect faces and show them.

Should this use verilook? It gives us eye positions...

CV can do that as well. What I need is:

For robot control don't need eyes or matching - just make sure it runs fast.
But need eyes for preprocessing / matching.

Now: make faster, need scaled down version.
Now: draw the rectangle(s) over video.
Stretching the contrast helped really well.
Run left/right eye cascades on the two regions (but cut them out of the original grey).

* * *

a sidetrack: make the mirror easier to readjust for different purposes.
[x] drop movie support
[x] (compile time) configurable tick() handler
    - should really be a class that has an incoming frame slot and can connect to the canvas
[ ] tick() handler dependent controls
    [ ] let the filter set up controls it wants

football players tracking:

1. background learning
    - simplest: a movie of the empty field
        - not always possible
        - will change with time (sun, clouds)
    - statistical:
        - collect histograms of image tiles for some time
        - for each tile: find similar histograms that occur the oftenest, assume that to be the background
            - this requires automatic classification of the histograms
    - dynamic
        - start with initial background (e.g. all green, or a single shot of empty field or statistically collected)
        - after blob detection use remainder of the image to update the tile histograms
            - see codebooks (p. 278)
2. detect areas different from the background
    - given tile histogram generate backprojection
    - combine all tile's backprojections to a single probability image
    - threshold / morphological operations
3. find blob shapes
    - contours of "white" areas
4. calculate histograms inside the blobs
5. classify the blobs based on area / histogram
6. blob location
    - use mid-lowest point as blob coordinates (in case of a single player corresponds to feet at the ground level)
    - rectify / unproject based on 4 points

Extra considerations:
- cvPyrMeanShiftFiltering is slow but simplifies the image a lot. helps to keep the histograms distinct. when to use it? may be to generate the inital histograms.
- the furthest background (in the test video) can be problematic, but as long as their uniforms are bright and in contrast with the background it's ok
- it would be clever to perspective-correct image when scaling down: minimize amount of processing for the foreground and loss of detail for the background.
    - TODO find which opencv geometric transformation functions does it, there is one that can do both in one go.

* * *

A flexible way to do video exploration:

[x] a VisionFilter registers a number of cv::Mat's in a dictionary
[x] it uses them directly because they are also its fields
[x] default 'incomingFrame()' does:
    1. store the incoming in the 'input' slot
    2. calls virtual filter() function
    3. displays selected slot

* * *

stagewise refactoring towards modular filter support:

[x] split current filter implementation into two classes - base filter class and football tracker
[x] split the face tracker part as well - including its gui
[x] remove the layer menu items
[x] remove the toolbar
[x] digit keys to stage selector: add key event handler to main window and react on digits there

